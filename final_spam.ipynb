{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8a2e5d-d6a1-436c-b99e-05d94fda030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything import well\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "print(\"everything import well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b42fb5-e51e-4c9e-86d2-568533940b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f3ce18-c2d2-46b1-82ff-4a0603d9d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stopwords are available\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# English stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#Somali stopwords (custom â€“ can be expanded carefully)\n",
    "somali_stopwords = {\n",
    "    \"iyo\", \"waa\", \"in\", \"ka\", \"ku\", \"si\", \"ayaa\", \"ma\", \"haa\", \"leh\", \"loo\",\n",
    "    \"la\", \"u\", \"wax\", \"badan\", \"ahay\", \"karo\", \"mid\", \"kuma\", \"wuu\", \"waxa\"\n",
    "}\n",
    "\n",
    "# Combine stopwords\n",
    "all_stopwords = english_stopwords.union(somali_stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenize (simple split)\n",
    "    words = text.split()\n",
    "    \n",
    "    # 4. Remove English + Somali stopwords\n",
    "    words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    # 5. Join words back to text\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6942754c-77e4-47d1-b9ad-7c50324b4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"extended_spam_dataset.csv\")\n",
    "df.columns = [\"label\", \"message\"]                       # ensure correct format\n",
    "df = df[df[\"label\"].isin([\"ham\", \"spam\"])]             # keep only valid labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173bb795-ecaa-4126-b593-cf983964349d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22 days to kick off! For Euro2004 U will be ke...</td>\n",
       "      <td>days kick euro kept date latest news results d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dai what this da.. Can i send my resume to thi...</td>\n",
       "      <td>dai da send resume id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al he does is moan at me if n e thin goes wron...</td>\n",
       "      <td>al moan n e thin goes wrong faultal de argumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maanta waan mashquulsanahay.</td>\n",
       "      <td>maanta waan mashquulsanahay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm in class. Will holla later</td>\n",
       "      <td>im class holla later</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  22 days to kick off! For Euro2004 U will be ke...   \n",
       "1  Dai what this da.. Can i send my resume to thi...   \n",
       "2  Al he does is moan at me if n e thin goes wron...   \n",
       "3                       Maanta waan mashquulsanahay.   \n",
       "4                     I'm in class. Will holla later   \n",
       "\n",
       "                                       clean_message  \n",
       "0  days kick euro kept date latest news results d...  \n",
       "1                              dai da send resume id  \n",
       "2  al moan n e thin goes wrong faultal de argumen...  \n",
       "3                        maanta waan mashquulsanahay  \n",
       "4                               im class holla later  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_message'] = df['message'].apply(clean_text)\n",
    "df[['message', 'clean_message']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7feea311-6928-44e9-a2c8-21015d083d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    5325\n",
       "1    1247\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99eccef9-ebdf-4224-a692-44b6aba89e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['clean_message']   # use consistent column name\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41c5f8d-4fc4-4e56-ade1-7e2f55de3b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution:\n",
      "label\n",
      "0    4260\n",
      "1     997\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test distribution:\n",
      "label\n",
      "0    1065\n",
      "1     250\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nTest distribution:\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab720298-5f20-48ad-8d5c-f0ddd3a65579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5257, 6000)\n",
      "(1315, 6000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_features=6000\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train_vec.shape)\n",
    "print(X_test_vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3924d98b-dc52-4b9d-9312-4bb2f5bce2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM Accuracy ===\n",
      "0.9863117870722433\n",
      "\n",
      "=== SVM Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1065\n",
      "           1       0.97      0.96      0.96       250\n",
      "\n",
      "    accuracy                           0.99      1315\n",
      "   macro avg       0.98      0.98      0.98      1315\n",
      "weighted avg       0.99      0.99      0.99      1315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 6A. TRAIN SVM MODEL\n",
    "# ==============================\n",
    "svm_model = SVC(\n",
    "    kernel=\"linear\",\n",
    "    probability=True,\n",
    "    class_weight=\"balanced\",\n",
    "    C=2\n",
    ")\n",
    "\n",
    "\n",
    "svm_model.fit(X_train_vec, y_train)\n",
    "\n",
    "\n",
    "svm_pred = svm_model.predict(X_test_vec)\n",
    "\n",
    "print(\"\\n=== SVM Accuracy ===\")\n",
    "print(accuracy_score(y_test, svm_pred))\n",
    "\n",
    "print(\"\\n=== SVM Report ===\")\n",
    "print(classification_report(y_test, svm_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5967ae1-ff19-4540-91a6-4d71791cabcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[211]\tvalid_0's binary_logloss: 0.0773083\n",
      "\n",
      "=== LightGBM Accuracy ===\n",
      "0.9787072243346008\n",
      "\n",
      "=== LightGBM Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1065\n",
      "           1       0.95      0.94      0.94       250\n",
      "\n",
      "    accuracy                           0.98      1315\n",
      "   macro avg       0.97      0.96      0.97      1315\n",
      "weighted avg       0.98      0.98      0.98      1315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 6B. TRAIN LIGHTGBM MODEL (CORRECT)\n",
    "# ==============================\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# LightGBM datasets (labels already numeric)\n",
    "lgb_train = lgb.Dataset(X_train_vec, label=y_train)\n",
    "lgb_eval = lgb.Dataset(X_test_vec, label=y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 40,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "lightgbm_model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=300,\n",
    "    valid_sets=[lgb_eval],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=30)]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "lgb_pred_prob = lightgbm_model.predict(X_test_vec)\n",
    "lgb_pred = (lgb_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== LightGBM Accuracy ===\")\n",
    "print(accuracy_score(y_test, lgb_pred))\n",
    "\n",
    "print(\"\\n=== LightGBM Report ===\")\n",
    "print(classification_report(y_test, lgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49b795bc-289f-4b8d-8b37-6320429463a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 7. SAVE MODELS\n",
    "# ==============================\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "lightgbm_model.save_model(\"lightgbm_model.txt\")\n",
    "\n",
    "print(\"\\nModels saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65cc152a-c63f-487a-aa0f-5861defd6c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer loaded successfully âœ…\n",
      "\n",
      "Original Message:\n",
      "salaman asxbta ina fcn tihin rajena beri ina dhibcaha kaso qadano fuad aan isku dayno oo shaqadi malin dhawed la rabay aan geyno\n",
      "\n",
      "Cleaned Message:\n",
      "salaman asxbta ina fcn tihin rajena beri ina dhibcaha kaso qadano fuad aan isku dayno oo shaqadi malin dhawed rabay aan geyno\n",
      "\n",
      "ðŸš¨Confidence score:\n",
      "âœ… HAM (0.98)\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ==============================\n",
    "import re\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ==============================\n",
    "# 2. ENSURE STOPWORDS\n",
    "# ==============================\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "# ==============================\n",
    "# 3. STOPWORDS (ENGLISH + SOMALI)\n",
    "# ==============================\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "somali_stopwords = {\n",
    "    \"iyo\", \"waa\", \"in\", \"ka\", \"ku\", \"si\", \"ayaa\", \"ma\", \"haa\", \"leh\", \"loo\",\n",
    "    \"la\", \"u\", \"wax\", \"badan\", \"ahay\", \"karo\", \"mid\", \"kuma\", \"wuu\", \"waxa\"\n",
    "}\n",
    "\n",
    "all_stopwords = english_stopwords.union(somali_stopwords)\n",
    "\n",
    "# ==============================\n",
    "# 4. CLEAN TEXT FUNCTION\n",
    "# ==============================\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in all_stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# ==============================\n",
    "# 5. LOAD SAVED MODEL & VECTORIZER\n",
    "# ==============================\n",
    "svm_model = joblib.load(\"svm_model.pkl\")\n",
    "vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer loaded successfully âœ…\")\n",
    "\n",
    "# ==============================\n",
    "# 6. TEST MESSAGE\n",
    "# ==============================\n",
    "test_message = \"salaman asxbta ina fcn tihin rajena beri ina dhibcaha kaso qadano fuad aan isku dayno oo shaqadi malin dhawed la rabay aan geyno\"\n",
    "\n",
    "# Clean text\n",
    "cleaned_message = clean_text(test_message)\n",
    "\n",
    "# Vectorize\n",
    "message_vec = vectorizer.transform([cleaned_message])\n",
    "\n",
    "# Predict\n",
    "prediction = svm_model.predict(message_vec)[0]\n",
    "probabilities = svm_model.predict_proba(message_vec)[0]\n",
    "\n",
    "spam_prob = probabilities[1]\n",
    "ham_prob = probabilities[0]\n",
    "\n",
    "# ==============================\n",
    "# 7. OUTPUT RESULT (CLEAR & CORRECT)\n",
    "# ==============================\n",
    "print(\"\\nOriginal Message:\")\n",
    "print(test_message)\n",
    "\n",
    "print(\"\\nCleaned Message:\")\n",
    "print(cleaned_message)\n",
    "\n",
    "print(\"\\nðŸš¨Confidence score:\")\n",
    "if prediction == 1:\n",
    "    print(f\"ðŸš¨ SPAM ({spam_prob:.2f})\")\n",
    "else:\n",
    "    print(f\"âœ… HAM ({ham_prob:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f643b9-ae5e-4123-972f-17d742e7f8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
